{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#terminal\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition\n",
    "import tempfile\n",
    "from gtts import gTTS\n",
    "from pygame import mixer\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn speech to sentences\n",
    "def listenTo():\n",
    "    # input from mic\n",
    "    r = speech_recognition.Recognizer() \n",
    "    with speech_recognition.Microphone() as source:\n",
    "        print(\"稍等\") \n",
    "        r.energy_threshold = 4000\n",
    "        r.adjust_for_ambient_noise(source, duration=1) \n",
    "        print('你想說什麼')\n",
    "        speak(\"你想說什麼\")\n",
    "        audio = r.listen(source)\n",
    "\n",
    "    # transfer input to sentences.\n",
    "    try:\n",
    "        print(\"你說:\")\n",
    "        print(r.recognize_google(audio, language=\"zh-TW\"))\n",
    "        \n",
    "        if r.recognize_google(audio, language=\"zh-TW\")=='離開':\n",
    "            return('000-exit')\n",
    "        else:\n",
    "            return(r.recognize_google(audio, language=\"zh-TW\"))\n",
    "    except (speech_recognition.UnknownValueError, speech_recognition.RequestError):\n",
    "        print(\"講清楚點再講一次,或說‘離開’結束功能[l]\")\n",
    "        speak(\"講清楚點再講一次,或說,離開,結束功能[l]\")\n",
    "        tmp = listenTo()\n",
    "        while tmp is None:\n",
    "            tmp = listenTo()\n",
    "        return(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn sentences to speech\n",
    "def speak(sentence, lang='zh-tw'):\n",
    "    mixer.init()\n",
    "    with tempfile.NamedTemporaryFile(delete=True) as fp:\n",
    "        tts = gTTS(text=sentence, lang=lang)\n",
    "        tts.save(\"{}.mp3\".format(fp.name))\n",
    "        mixer.music.load('{}.mp3'.format(fp.name))\n",
    "        mixer.music.play()\n",
    "        # continue other event after the music is done.\n",
    "        while mixer.music.get_busy() == True:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(sentence):\n",
    "    in_index = sentence.rfind(' in ')\n",
    "    if in_index==-1:\n",
    "        if len(sentence.split())<3:\n",
    "            in_index = sentence.lower().rfind(' weather')\n",
    "            print(sentence[:in_index].strip())\n",
    "            return(sentence[:in_index].strip())\n",
    "        else:\n",
    "            print(\"請只說地點\")\n",
    "            speak(\"請只說地點\", 'zh-tw')\n",
    "            tmp = listenTo()\n",
    "            while tmp is None:\n",
    "                tmp = listenTo()\n",
    "        return(tmp)\n",
    "    else:\n",
    "        print(sentence[in_index+3:].strip())\n",
    "        return(sentence[in_index+3:].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#weather = Weather(unit=Unit.CELSIUS)\n",
    "#location = weather.lookup_by_location('餔拉不拉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather import Weather, Unit\n",
    "\n",
    "# get weather information by weather api\n",
    "def get_weather(loc):\n",
    "    weather = Weather(unit=Unit.CELSIUS)\n",
    "    try:\n",
    "        location = weather.lookup_by_location(loc)\n",
    "        \n",
    "        if location is not None:\n",
    "            weather_cond = translator.translate(location.location.city, dest='zh-tw').text + \\\n",
    "                           '今天天氣為' + translator.translate(location.forecast[0].text, dest='zh-tw').text + \\\n",
    "                           ', 最高溫度為' + location.forecast[0].high + '度,最低溫度' + location.forecast[0].low + '度,' + \\\n",
    "                           '明天天氣為' + translator.translate(location.forecast[1].text, dest='zh-tw').text + \\\n",
    "                           ', 最高溫度為' + location.forecast[1].high + '度,最低溫度' + location.forecast[1].low + '度'\n",
    "            print(weather_cond)\n",
    "            speak(weather_cond)\n",
    "            #return(True)\n",
    "        else:\n",
    "            raise ValueError('no result return')\n",
    "    except (AttributeError, ValueError):\n",
    "        print(\"請只說地點,或說‘離開’結束功能[w]\")\n",
    "        speak(\"請只說地點,或說,離開,結束功能[w]\")\n",
    "        #return(None)\n",
    "        #get_weather(listenTo())\n",
    "        tmp = listenTo()\n",
    "        if tmp=='000-exit':\n",
    "            print(\"好吧我走[w]\")\n",
    "            speak(\"好吧我走[w]\")\n",
    "            return(None)\n",
    "        else:\n",
    "            get_weather(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_yahoo_movie_info():    \n",
    "    ###\n",
    "    # first part\n",
    "    ###\n",
    "    # crawl down movie showtimes of vieshow cinemas at xinyi\n",
    "    xinyi_cinemas_url = 'https://movies.yahoo.com.tw/theater_result.html/id=45'\n",
    "    xinyi_page = urlopen(xinyi_cinemas_url)\n",
    "    xinyi_soup = BeautifulSoup(xinyi_page, 'html.parser')\n",
    "    theater_list = pd.DataFrame({\n",
    "        'name':[ele.get_text() for ele in xinyi_soup.select('.theaterlist_name a')],\\\n",
    "        'type':[ele.get_text() for ele in xinyi_soup.select('.theaterlist_name .tapR')],\\\n",
    "        'time':[re.sub('\\n', ',', ele.get_text().strip()) for ele in xinyi_soup.select('.theater_time')]\n",
    "    })\n",
    "    theater_list = theater_list[theater_list['type']=='數位']\n",
    "    ###\n",
    "    # second part\n",
    "    ###\n",
    "    # crawl down movie ranking from yahoo movies\n",
    "    cinemas_rank_url = 'https://movies.yahoo.com.tw/chart.html'\n",
    "    cinemas_rank_page = urlopen(cinemas_rank_url)\n",
    "    cinemas_rank_soup = BeautifulSoup(cinemas_rank_page, 'html.parser')\n",
    "    movie_score= pd.DataFrame({\n",
    "        'name':[cinemas_rank_soup.select('h1')[-1].get_text()] + [ele.get_text() for ele in cinemas_rank_soup.select('.rank_txt')],\n",
    "        'score':[ele.get_text() for ele in cinemas_rank_soup.select('.count')]\n",
    "    })\n",
    "    # merge movie info and showtimes\n",
    "    movie_list = pd.merge(theater_list, movie_score, on='name', how='right').dropna()\n",
    "    movie_list.score = pd.to_numeric(movie_list.score, errors='coerce')\n",
    "    #movie_list.sort_values('score', ascending=False).drop('type', 1)\n",
    "    return(movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_list = get_yahoo_movie_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "## recommand three movies from movie ranking data \n",
    "def random_movie():\n",
    "    tmp = random.sample(list(set(movie_list.name)), 3)\n",
    "    string = '給你隨機三部電影吧！' +\\\n",
    "        '1,' + tmp[0] + ',2,' + tmp[1] + ',3,' + tmp[2]\n",
    "    print(string)\n",
    "    speak(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r3dmaohong/anaconda3/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# search movie info which is in the rank list \n",
    "# using pinyin and fuzzy match to ignore typo in chinese\n",
    "movie_name_pinyin = [re.sub(' ', '', ''.join(lazy_pinyin(ele))) for ele in movie_list.name]\n",
    "def movie_info(listen_movie_name, speakout=True):\n",
    "    fuzzy_match_score = [fuzz.partial_ratio(''.join(lazy_pinyin(ele)), ''.join(lazy_pinyin(listen_movie_name))) for ele in movie_name_pinyin]\n",
    "    \n",
    "    if any(ele>60 for ele in fuzzy_match_score):\n",
    "        movie_name_input = movie_list.name[fuzzy_match_score.index(max(fuzzy_match_score))]\n",
    "        string = movie_name_input + ', 評分為' + re.sub('\\\\.', '點', str(movie_list[movie_list.name==movie_name_input]['score'].iloc[0])) + \\\n",
    "                 '分, 信義威秀今天上映時間為' + re.sub(':', '點', ',另一廳的時間為,'.join(movie_list[movie_list.name==movie_name_input]['time']))\n",
    "        if speakout==True:\n",
    "            print(string)\n",
    "            speak(string)\n",
    "        else:\n",
    "            return(True)\n",
    "    else:\n",
    "        if speakout==True:\n",
    "            print('查不到這電影啦')\n",
    "            speak('查不到這電影啦')\n",
    "        else:\n",
    "            return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r3dmaohong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# training data\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "words = [] # words list\n",
    "classes = [] # what kind of function the ai has?\n",
    "documents = [] # documents after tokenized and its label\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "# unique\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # one hot encoding the label\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# sort it randomly\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# training input and output (label)\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.00500\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1000 | loss: 0.00500 - acc: 1.0000 -- iter: 08/14\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.00506\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.00506 - acc: 1.0000 -- iter: 14/14\n",
      "--\n",
      "INFO:tensorflow:/Users/r3dmaohong/workspace/robots/aaron-personal-assistant/arron.model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('arron.model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/r3dmaohong/workspace/robots/aaron-personal-assistant/arron.model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./arron.model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return(sentence_words)\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any of the probabilities in the result is lower than error_threshold, don't extract it.\n",
    "ERROR_THRESHOLD = 0.3\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memorized previous question type\n",
    "previous_status=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "天氣如何\n",
      "請只說地點\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "台北\n",
      "台北市今天天氣為雷暴, 最高溫度為30度,最低溫度22度,明天天氣為雷暴, 最高溫度為31度,最低溫度23度\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "責任推薦我的電影好了\n",
      "好吧推薦電影給你\n",
      "給你隨機三部電影吧！1,糯爾摩斯,2,一級玩家,3,雨妳再次相遇\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "我要查電影一級玩家的相關資訊\n",
      "喔你想查電影喔\n",
      "一級玩家, 評分為4點7分, 信義威秀今天上映時間為09點30,10點50,11點30,12點10,13點30,14點50,16點10,16點50,17點30,18點50,20點10,21點30,22點50,00點10,00點50,01點30\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "講清楚點再講一次,或說‘離開’結束功能[l]\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "我想知道洪雪嗎\n",
      "喔你想查電影喔\n",
      "紅雀, 評分為4點4分, 信義威秀今天上映時間為21點05\n",
      "稍等\n",
      "你想說什麼\n",
      "你說:\n",
      "離開\n",
      "好吧我走\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    string = listenTo()\n",
    "    translator = Translator()\n",
    "    input_status = classify(translator.translate(string, dest='en').text)\n",
    "    \n",
    "    if string=='000-exit':\n",
    "        print(\"好吧我走\")\n",
    "        speak(\"好吧我走\")\n",
    "        break\n",
    "    \n",
    "    if input_status=='movie_info':\n",
    "        print('喔你想查電影喔')\n",
    "        speak('喔你想查電影喔')\n",
    "        movie_info(string)\n",
    "    elif ((previous_status=='movie_info')) & (movie_info(string, speakout=False)):\n",
    "        print('喔你想查電影喔')\n",
    "        speak('喔你想查電影喔')\n",
    "        movie_info(string)\n",
    "    elif input_status=='random_movie':\n",
    "        print('好吧推薦電影給你')\n",
    "        speak('好吧推薦電影給你')\n",
    "        random_movie()\n",
    "    elif input_status=='weather':\n",
    "        if string=='000-exit':\n",
    "            pass\n",
    "        else:\n",
    "            en_input_string = translator.translate(string, dest='en').text\n",
    "            loc = extract_location(en_input_string)\n",
    "            get_weather(loc)\n",
    "    else:\n",
    "        print('不知道你在說啥')\n",
    "        speak('不知道你在說啥')\n",
    "    previous_status = input_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_match_score.index(max(fuzzy_match_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[''.join(lazy_pinyin(ele)) for ele in movie_list.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak('11:50,14:00,16:10,20:20,22:30,00:40', 'zh-tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinyi_soup.findAll('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xinyi_soup.select('.theaterlist_name a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "listenTo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speak(get_weather('taipei'), 'zh-tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.translate('台北', dest='zh-tw').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_location(en_input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weather import Weather, Unit\n",
    "weather = Weather(unit=Unit.CELSIUS)\n",
    "\n",
    "location = weather.lookup_by_location('djsjdisjd')\n",
    "location.condition.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location.forecast[0].date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('最高溫度', location.forecast[0].high, '最低溫度', location.forecast[0].low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location.print_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
